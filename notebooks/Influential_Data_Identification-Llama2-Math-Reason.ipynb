{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51542ae2",
   "metadata": {},
   "source": [
    "# Influential data identification - Llama2 - Math - Reason\n",
    "\n",
    "This notebook demonstrates how to efficiently compute the influence functions using DataInf, showing its application to **influential data identification** tasks.\n",
    "\n",
    "- Model: [llama-2-13b-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) trained on a mix of publicly available online datasets.\n",
    "- Fine-tuning dataset: Synthetic Math Problem (with reasoning) dataset\n",
    "\n",
    "References\n",
    "- `trl` HuggingFace library [[Link]](https://github.com/huggingface/trl).\n",
    "- DataInf is available at this [ArXiv link](https://arxiv.org/abs/2310.00902)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759f0a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kuacc/users/oince22/.conda/envs/datainf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from lora_model import LORAEngineGeneration\n",
    "from influence import IFEngineGeneration\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e0c14",
   "metadata": {},
   "source": [
    "## Fine-tune a model\n",
    "- We fine-tune a llama-2-13b-chat model on the `math problem (with reasoning)` dataset. We use `src/sft_trainer.py`, which is built on HuggingFace's [SFTTrainer](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py). It will take around 30 minutes.\n",
    "- For the `sentence transformation` and `math problem (without reasoning)` datasets, please replace `math_with_reason_train` with `grammars_train` or `math_without_reason_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c18e286-c154-483f-9872-a431447ea16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 583/583 [00:00<00:00, 1.32MB/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNousResearch/Llama-2-7b-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNousResearch/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/datainf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/datainf/lib/python3.10/site-packages/transformers/modeling_utils.py:2614\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit \u001b[38;5;129;01mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   2613\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m-> 2614\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2615\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2616\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pip install bitsandbytes` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2618\u001b[0m         )\n\u001b[1;32m   2620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2621\u001b[0m         \u001b[38;5;66;03m# We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\u001b[39;00m\n\u001b[1;32m   2622\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2623\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverriding torch_dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with `torch_dtype=torch.float16` due to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2624\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2625\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2626\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch_dtype=torch.float16 to remove this warning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2627\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\", load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d569f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/kuacc/users/oince22/.conda/envs/datainf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/kuacc/users/oince22/hpc_run/DataInf/src/sft_trainer.py\", line 88, in <module>\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"/kuacc/users/oince22/.conda/envs/datainf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 487, in from_pretrained\n",
      "    resolved_config_file = cached_file(\n",
      "  File \"/kuacc/users/oince22/.conda/envs/datainf/lib/python3.10/site-packages/transformers/utils/hub.py\", line 429, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/kuacc/users/oince22/.conda/envs/datainf/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/kuacc/users/oince22/.conda/envs/datainf/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 158, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kuacc/users/oince22/hpc_run/OPT/llama-2-7b-chat'. Use `repo_type` argument if needed.\n"
     ]
    }
   ],
   "source": [
    "!python /kuacc/users/oince22/hpc_run/DataInf/src/sft_trainer.py \\\n",
    "    --model_name /kuacc/users/oince22/hpc_run/OPT/llama-2-7b-chat \\\n",
    "    --dataset_name /kuacc/users/oince22/hpc_run/DataInf/datasets/math_with_reason_train.hf \\\n",
    "    --output_dir /kuacc/users/oince22/hpc_run/DataInf/models/math_with_reason_7bf \\\n",
    "    --dataset_text_field text \\\n",
    "    --load_in_8bit \\\n",
    "    --use_peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdb7d3",
   "metadata": {},
   "source": [
    "## Load a fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b732140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961e55e5efd04cef8b9955f06336aab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Please change the following objects to  \"YOUR-LLAMA-PATH\" and \"YOUR-DATAINF-PATH\"\n",
    "base_path = \"/kuacc/users/oince22/hpc_run/OPT/llama-2-7b-chat\" \n",
    "project_path =\"/kuacc/users/oince22/hpc_run/DataInf\" \n",
    "lora_engine = LORAEngineGeneration(base_path=base_path, \n",
    "                                   project_path=project_path,\n",
    "                                   dataset_name='math_with_reason')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df744c",
   "metadata": {},
   "source": [
    "### Example: model prediction\n",
    "The following prompt has not been seen during the fine-tuning process, although there are many similar addition problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cdd18c-09b1-491e-8818-0dd2d690e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db75df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Print Input prompt\n",
      "\n",
      "Emily scored 10 points in the first game, 30 points in the second, 100 in the third, and 20 in the fourth game. What is her total points? Output only the answer.\n",
      "\n",
      "--------------------------------------------------\n",
      "Print Model output\n",
      "\n",
      "Emily scored 10 points in the first game, 30 points in the second, 100 in the third, and 20 in the fourth game. What is her total points? Output only the answer.\n",
      "\n",
      "Answer: 160\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Emily scored 10 points in the first game, 30 points in the second, 100 in the third, and 20 in the fourth game. What is her total points? Output only the answer.\n",
    "\"\"\"\n",
    "inputs = lora_engine.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = lora_engine.model.generate(input_ids=inputs.input_ids, \n",
    "                                          max_length=128,\n",
    "                                          pad_token_id=lora_engine.tokenizer.eos_token_id)\n",
    "output = lora_engine.tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")[0]\n",
    "\n",
    "print('-'*50)\n",
    "print('Print Input prompt')\n",
    "print(prompt)\n",
    "print('-'*50)\n",
    "print('Print Model output')\n",
    "print(output)\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb829f",
   "metadata": {},
   "source": [
    "## Compute the gradient\n",
    " - Influence function uses the first-order gradient of a loss function. Here we compute gradients using `compute_gradient`\n",
    " - `tr_grad_dict` has a nested structure of two Python dictionaries. The outer dictionary has `{an index of the training data: a dictionary of gradients}` and the inner dictionary has `{layer name: gradients}`. The `val_grad_dict` has the same structure but for the validationd data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d34884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function LORAEngineGeneration.create_tokenized_datasets.<locals>.<lambda> at 0x15543fe432e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235a7c1aaba54c018771b84bdcbc64fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3dd52c099b4fadb389c14b1e439f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 900/900 [09:48<00:00,  1.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets, collate_fn = lora_engine.create_tokenized_datasets()\n",
    "tr_grad_dict, val_grad_dict = lora_engine.compute_gradient(tokenized_datasets, collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f47b2ec",
   "metadata": {},
   "source": [
    "## Compute the influence function\n",
    " - We compute the inverse Hessian vector product first using `compute_hvps()`. With the argument `compute_accurate=True`, the exact influence function value will be computed. (it may take an hour to compute).\n",
    "<!--  - Here, we take a look at the first five validation data points. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94703c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [36:18<00:00, 21.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing IF for method:  identity\n",
      "Computing IF for method:  proposed\n"
     ]
    }
   ],
   "source": [
    "influence_engine = IFEngineGeneration()\n",
    "influence_engine.preprocess_gradients(tr_grad_dict, val_grad_dict)\n",
    "influence_engine.compute_hvps()\n",
    "influence_engine.compute_IF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f625e",
   "metadata": {},
   "source": [
    "## Attributes of influence_engine\n",
    "There are a couple of useful attributes in `influence_engine`. For intance, to compare the runtime, one can use `time_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7321f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'identity': 3.814697265625e-06, 'proposed': 2178.7828104496})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influence_engine.time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "085e0d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['identity', 'proposed'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influence_engine.IF_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cc0dc",
   "metadata": {},
   "source": [
    "## Application to influential data detection task\n",
    "- We inspect the most and the least influential data points for validation data loss. Here, the most (and the least) influential data points are determined by the absolute value of influence function values.\n",
    "- Why? the least influential data points will have near zero values, which means the training data point does not affect the validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f588d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_influential_data_point_proposed=influence_engine.IF_dict['proposed'].apply(lambda x: x.abs().argmax(), axis=1)\n",
    "least_influential_data_point_proposed=influence_engine.IF_dict['proposed'].apply(lambda x: x.abs().argmin(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9bef98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Sample ID: 21\n",
      " Solve the following math problem. In an aquarium, there are 67 sharks and 11 dolphins. If they bought 2 more sharks, how many sharks would be there in total? -> Reason: Total sharks = 67 + 2. Answer: 69</s> \n",
      "\n",
      "The most influential training sample: \n",
      " Solve the following math problem. In an aquarium, there are 79 sharks and 56 dolphins. If they bought 2 more sharks, how many sharks would be there in total? -> Reason: Total sharks = 79 + 2. Answer: 81</s> \n",
      "\n",
      "The least influential training sample: \n",
      " Solve the following math problem. Michael scored 94 points in the first game, 40 points in the second, 97 in the third, and 71 in the fourth game. What is his total points? -> Reason: Total points = 94 + 40 + 97 + 71. Answer: 302</s>\n"
     ]
    }
   ],
   "source": [
    "val_id=21\n",
    "print(f'Validation Sample ID: {val_id}\\n', \n",
    "      lora_engine.validation_dataset[val_id]['text'], '\\n')\n",
    "print('The most influential training sample: \\n', \n",
    "      lora_engine.train_dataset[int(most_influential_data_point_proposed.iloc[val_id])]['text'], '\\n')\n",
    "print('The least influential training sample: \\n', \n",
    "      lora_engine.train_dataset[int(least_influential_data_point_proposed.iloc[val_id])]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7692d6c",
   "metadata": {},
   "source": [
    "# AUC and Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39b4915e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity AUC: 0.772/0.173\n",
      "proposed AUC: 1.000/0.001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "identity_df=influence_engine.IF_dict['identity']\n",
    "proposed_df=influence_engine.IF_dict['proposed']\n",
    "\n",
    "n_train, n_val = 900, 100\n",
    "n_sample_per_class = 90 \n",
    "n_class = 10\n",
    "\n",
    "identity_auc_list, proposed_auc_list=[], []\n",
    "for i in range(n_val):\n",
    "    gt_array=np.zeros(n_train)\n",
    "    gt_array[(i//n_class)*n_sample_per_class:((i//n_class)+1)*n_sample_per_class]=1\n",
    "    \n",
    "    # The influence function is anticipated to have a big negative value when its class equals to a validation data point. \n",
    "    # This is because a data point with the same class is likely to be more helpful in minimizing the validation loss.\n",
    "    # Thus, we multiply the influence function value by -1 to account for alignment with the gt_array. \n",
    "    identity_auc_list.append(roc_auc_score(gt_array, -(identity_df.iloc[i,:].to_numpy())))\n",
    "    proposed_auc_list.append(roc_auc_score(gt_array, -(proposed_df.iloc[i,:].to_numpy())))\n",
    "    \n",
    "print(f'identity AUC: {np.mean(identity_auc_list):.3f}/{np.std(identity_auc_list):.3f}')\n",
    "print(f'proposed AUC: {np.mean(proposed_auc_list):.3f}/{np.std(proposed_auc_list):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baa20253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity Recall: 0.258/0.388\n",
      "proposed Recall: 0.996/0.025\n"
     ]
    }
   ],
   "source": [
    "# Recall calculations\n",
    "identity_recall_list, proposed_recall_list=[], []\n",
    "for i in range(n_val):\n",
    "    correct_label = i // 10\n",
    "\n",
    "    # Similar to AUC computation, we consider the first 90 data points with the smallest influence function values \n",
    "    # These data points with the smallest influence function values likely have the same class with the validation data point.\n",
    "    sorted_labels = np.argsort(identity_df.iloc[i].values)// 90 \n",
    "    recall_identity = np.count_nonzero(sorted_labels[0:90] == correct_label) / 90.0\n",
    "    identity_recall_list.append(recall_identity)\n",
    "    \n",
    "    sorted_labels = np.argsort(proposed_df.iloc[i].values)// 90 \n",
    "    recall_proposed = np.count_nonzero(sorted_labels[0:90] == correct_label) / 90.0\n",
    "    proposed_recall_list.append(recall_proposed)\n",
    "    \n",
    "print(f'identity Recall: {np.mean(identity_recall_list):.3f}/{np.std(identity_recall_list):.3f}')\n",
    "print(f'proposed Recall: {np.mean(proposed_recall_list):.3f}/{np.std(proposed_recall_list):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1da851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4fab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
